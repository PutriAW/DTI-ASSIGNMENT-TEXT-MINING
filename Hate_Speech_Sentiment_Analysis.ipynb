{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hate Speech Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO//+P2v48+mS2tiMoDUYek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PutriAW/DTI-ASSIGNMENT-TEXT-MINING/blob/main/Hate_Speech_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foers9b76Shz"
      },
      "source": [
        "## **Hate Speech Sentiment Analysis**\n",
        "Created By Putri Apriyanti Windya \n",
        " (DS0124 - Data Scientist 01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI2nFDrm6XHp"
      },
      "source": [
        "# **Dataset**\n",
        "\n",
        "---\n",
        " Dataset for this classification obtained from https://raw.githubusercontent.com/ialfina/id-hatespeech-detection/master/IDHSD_RIO_unbalanced_713_2017.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m9VAiQw6qnn"
      },
      "source": [
        "# **Description**\n",
        "\n",
        "---\n",
        "\n",
        "The Dataset for Hate Speech Detection in Indonesian\n",
        "(Dataset untuk Deteksi Ujaran Kebencian dalam Bahasa Indonesia)\n",
        "\n",
        "Dataset\n",
        "The dataset is a two columns data of: label - tweet, consist of 713 tweets in Indonesian.\n",
        "The label is Non_HS or HS. Non_HS for \"non-hate-speech\" tweet and HS for \"hate-speech\" tweet.\n",
        "\n",
        "Number of Non_HS tweets: 453\n",
        "Number of HS tweets: 260\n",
        "Since this dataset is unbalanced, you might have to do over-sampling/down-sampling in order to create a balanced dataset.\n",
        "The dataset may be used freely, but if you want to publish paper/publication using the dataset, please cite this publication:\n",
        "\n",
        "Ika Alfina, Rio Mulia, Mohamad Ivan Fanany, and Yudo Ekanata, \"Hate Speech Detection in Indonesian Language: A Dataset and Preliminary Study \", in Proceeding of 9th International Conference on Advanced Computer Science and Information Systems 2017(ICACSIS 2017)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3axp-EC7EwT"
      },
      "source": [
        "# **Problem to Solve**\n",
        "\n",
        "---\n",
        "\n",
        "Do sentiment Analysis to know whether a twitter tweet is hate speech or non hate speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rohc2De7vMl"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAP5a4Aj71MA"
      },
      "source": [
        "## **Data Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2naRgoNn795d"
      },
      "source": [
        "**Import All Libraries that Needed for Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luABDyAIhyVt",
        "outputId": "af59ffb9-6e49-4de4-830f-9d16f57ec237"
      },
      "source": [
        "# install library for indonesian language stemming\n",
        "!pip install Sastrawi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VBGoCLf8Cp7"
      },
      "source": [
        "# Import Library\n",
        "# text preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import requests\n",
        "import io\n",
        "import re # regular expression\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # stemming indonesian language\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Vectorization and splitting\n",
        "from sklearn.feature_extraction.text import CountVectorizer # to create Bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # tfid Vector \n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score # confussion matrix\n",
        "from sklearn.preprocessing import LabelEncoder # to convert classes to number \n",
        "from sklearn.model_selection import train_test_split  # for splitting data \n",
        "from sklearn.metrics import accuracy_score # to calculate accuracy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9-rw4pUzRUJ"
      },
      "source": [
        "# Get Data from Github\n",
        "result = requests.get('https://raw.githubusercontent.com/ialfina/id-hatespeech-detection/master/IDHSD_RIO_unbalanced_713_2017.txt')\n",
        "data = io.StringIO(result.text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "uFTPvTV6zZK0",
        "outputId": "63af06cb-9f9d-42cd-d67f-1525041def7b"
      },
      "source": [
        "# Convert result into data frame\n",
        "df_hs = pd.read_csv(data, sep='\\t')\n",
        "df_hs.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label                                              Tweet\n",
              "0  Non_HS  RT @spardaxyz: Fadli Zon Minta Mendagri Segera...\n",
              "1  Non_HS  RT @baguscondromowo: Mereka terus melukai aksi...\n",
              "2  Non_HS  Sylvi: bagaimana gurbernur melakukan kekerasan...\n",
              "3  Non_HS  Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...\n",
              "4  Non_HS  RT @lisdaulay28: Waspada KTP palsu.....kawal P..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ouZkCsO23O84",
        "outputId": "781fe89d-c7a6-4583-991b-3a69712cec5c"
      },
      "source": [
        "# Count HS and Non_HS label\n",
        "df_hs.Label.value_counts().to_frame()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Non_HS</th>\n",
              "      <td>453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HS</th>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Label\n",
              "Non_HS    453\n",
              "HS        260"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oba8U79I3PhJ"
      },
      "source": [
        "## **Text Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ClhwUSB3ZV3"
      },
      "source": [
        "**Case folding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "rYnp4pXtk_mW",
        "outputId": "62776793-103a-451f-f3d5-fe1da0f4fc7f"
      },
      "source": [
        "temp_tweet = []\n",
        "\n",
        "for tw in df_hs['Tweet']:\n",
        "  # removal of @name[mention]\n",
        "  tw = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", tw)\n",
        "\n",
        "  # removal of links[https://blabala.com]\n",
        "  # tw = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", tw)\n",
        "  tw = re.sub(r\"http\\S+\", \"\", tw)\n",
        "\n",
        "  # removal of new line\n",
        "  tw = re.sub('\\n', '', tw)\n",
        "\n",
        "  # removal of RT\n",
        "  tw = re.sub('RT', '', tw)\n",
        "\n",
        "  # Tokenization\n",
        "  # removal of punctuations and numbers\n",
        "  tw = re.sub(\"[^a-zA-Z^']\", \" \", tw)\n",
        "  tw = re.sub(\" {2,}\", \" \", tw)\n",
        "\n",
        "  # remove leading and trailing whitespace\n",
        "  tw = tw.strip()\n",
        "\n",
        "  # remove whitespace with a single space\n",
        "  tw = re.sub(r'\\s+', ' ', tw)\n",
        "\n",
        "  # convert text to Lowercase\n",
        "  tw = tw.lower();\n",
        "  temp_tweet.append(tw)\n",
        "\n",
        "df_hs['Clean_Tweet'] = temp_tweet\n",
        "df_hs.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Clean_Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>fadli zon minta mendagri segera menonaktifkan ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>mereka terus melukai aksi dalam rangka memenja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>sylvi bagaimana gurbernur melakukan kekerasan ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>ahmad dhani tak puas debat pilkada masalah jal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>waspada ktp palsu kawal pilkada</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                        Clean_Tweet\n",
              "0  Non_HS  ...  fadli zon minta mendagri segera menonaktifkan ...\n",
              "1  Non_HS  ...  mereka terus melukai aksi dalam rangka memenja...\n",
              "2  Non_HS  ...  sylvi bagaimana gurbernur melakukan kekerasan ...\n",
              "3  Non_HS  ...  ahmad dhani tak puas debat pilkada masalah jal...\n",
              "4  Non_HS  ...                    waspada ktp palsu kawal pilkada\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVzTbkX89E0j"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6_9yADs9EAx"
      },
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem(tweet) :\n",
        "    hasil = stemmer.stem(tweet)\n",
        "    return hasil\n",
        "\n",
        "df_hs['Clean_Tweet'] = df_hs.apply(lambda row : stem(row['Clean_Tweet']), axis = 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKMHtUFp9YTv"
      },
      "source": [
        "**Stop Word Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TaYNmVm9Ys7"
      },
      "source": [
        "R_factory = StopWordRemoverFactory()\n",
        "R_stopword = R_factory.create_stop_word_remover()\n",
        "\n",
        "def R_stopwords(tweet) :\n",
        "    tweet = tweet.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "    return R_stopword.remove(tweet)\n",
        "\n",
        "df_hs['Clean_Tweet'] = df_hs.apply(lambda row : stem(row['Clean_Tweet']), axis = 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "F9utzdcA-DYR",
        "outputId": "8dc9911f-46a4-45fb-bb71-07156d106a1e"
      },
      "source": [
        "df_hs.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Clean_Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>fadli zon minta mendagri segera nonaktif ahok ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>mereka terus luka aksi dalam rangka penjara ah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>sylvi bagaimana gurbernur laku keras perempuan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>ahmad dhani tak puas debat pilkada masalah jal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>waspada ktp palsu kawal pilkada</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                        Clean_Tweet\n",
              "0  Non_HS  ...  fadli zon minta mendagri segera nonaktif ahok ...\n",
              "1  Non_HS  ...  mereka terus luka aksi dalam rangka penjara ah...\n",
              "2  Non_HS  ...  sylvi bagaimana gurbernur laku keras perempuan...\n",
              "3  Non_HS  ...  ahmad dhani tak puas debat pilkada masalah jal...\n",
              "4  Non_HS  ...                    waspada ktp palsu kawal pilkada\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuRAspZxih3L"
      },
      "source": [
        "## **Vectorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyKVjWkqkgaj"
      },
      "source": [
        "**Count Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDcOHVFOgsbv",
        "outputId": "c8c8151d-cf1c-4e66-9e42-c2f8116dedca"
      },
      "source": [
        "X = df_hs['Clean_Tweet']\n",
        "# Count Vectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vector = count_vectorizer.fit_transform(X)\n",
        "count_vector.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(713, 2291)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wNiYiXHkDVU"
      },
      "source": [
        "# Show Vocabulary\n",
        "# count_vectorizer.vocabulary_"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT1PyMaTkfM2"
      },
      "source": [
        "**TF-IDF Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PlA3rMBkP2T",
        "outputId": "0e692e78-3cf2-4d4b-f4ea-c40e9ec0822e"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfid_vector = tfidf_vectorizer.fit_transform(X)\n",
        "tfid_vector.shape "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(713, 2291)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfjj9PmuoTKJ"
      },
      "source": [
        "**Vectorizer Conclusion**\n",
        "\n",
        "Based on advantages and disadvantages, i decided to chose TF-IDF rather than count vectorizer because TF-IDF has less disadvantages than count vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEXH23Tzmw9t"
      },
      "source": [
        "**Label Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzChS-fumvVQ",
        "outputId": "9c23b460-7ff4-4b40-bba1-ed3b2957241b"
      },
      "source": [
        "# Encode Target\n",
        "encoder = LabelEncoder()\n",
        "tweet_label = encoder.fit_transform(df_hs['Label'])\n",
        "tweet_label"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtCIQtkbl_U5"
      },
      "source": [
        "# **Data Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agf0A6ZjlmAb",
        "outputId": "b79554e9-8618-4852-ac2a-934d06342b47"
      },
      "source": [
        "# Set Training and Testing Data (70:30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfid_vector, tweet_label , shuffle = True, test_size=0.3, random_state=11)\n",
        "\n",
        "# Show the Training and Testing Data\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(499, 2291)\n",
            "(214, 2291)\n",
            "(499,)\n",
            "(214,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv-0mL6ggkca"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFx532-pXBu"
      },
      "source": [
        "## **Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7omAqg4pYDM"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svmc = SVC(kernel='rbf',probability=True)\n",
        "\n",
        "# Training SVM\n",
        "svmc.fit(X_train, y_train)\n",
        "\n",
        "# predict SVM to test data\n",
        "y_pred_svm = svmc.predict(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcnW5f6cp5Ig",
        "outputId": "4a8e74ba-2feb-4526-c29f-0b27e55f8985"
      },
      "source": [
        "# Show the Confussion Matrix\n",
        "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "cm_svm"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 37,  44],\n",
              "       [  4, 129]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMtcB8xyrX2i",
        "outputId": "9ab78c77-413d-4c78-908d-340d0bcf8b4e"
      },
      "source": [
        "# Classification report\n",
        "print(\"Report : \\n\", classification_report(y_test, y_pred_svm))\n",
        "print(\"Accuracy : \",accuracy_score(y_test,y_pred_svm))\n",
        "accSVMC = accuracy_score(y_test,y_pred_svm)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.46      0.61        81\n",
            "           1       0.75      0.97      0.84       133\n",
            "\n",
            "    accuracy                           0.78       214\n",
            "   macro avg       0.82      0.71      0.72       214\n",
            "weighted avg       0.81      0.78      0.75       214\n",
            "\n",
            "Accuracy :  0.7757009345794392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orp5LrGasOHt"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTTD6c_9sVFx"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lrc = LogisticRegression()\n",
        "\n",
        "# Training Logistic regression\n",
        "lrc.fit(X_train, y_train)\n",
        "\n",
        "# predict lrc to test data\n",
        "y_pred_lrc = lrc.predict(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG0I-nigto6z",
        "outputId": "ac4c1bca-2917-4267-afcd-9eddbe6f83c4"
      },
      "source": [
        "# Show the Confussion Matrix\n",
        "cm_lrc = confusion_matrix(y_test, y_pred_lrc)\n",
        "cm_lrc"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 31,  50],\n",
              "       [  2, 131]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TbkpeZOtsLw",
        "outputId": "1b3cff38-cf14-4569-c596-87b0e8cdf728"
      },
      "source": [
        "# Classification report\n",
        "print(\"Report : \\n\", classification_report(y_test, y_pred_lrc))\n",
        "print(\"Accuracy : \",accuracy_score(y_test,y_pred_lrc))\n",
        "accLRC = accuracy_score(y_test,y_pred_lrc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.38      0.54        81\n",
            "           1       0.72      0.98      0.83       133\n",
            "\n",
            "    accuracy                           0.76       214\n",
            "   macro avg       0.83      0.68      0.69       214\n",
            "weighted avg       0.81      0.76      0.72       214\n",
            "\n",
            "Accuracy :  0.7570093457943925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN1o-Tsvt8NT"
      },
      "source": [
        "## **Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq0l22ixt7bi"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 2, random_state = 0)\n",
        "\n",
        "# Training gbc\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# predict gbc to test data\n",
        "y_pred_gbc = gbc.predict(X_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KviQjf1hu9et",
        "outputId": "294681d7-9d95-4789-bd32-937456d36337"
      },
      "source": [
        "# Show the Confussion Matrix\n",
        "cm_gbc = confusion_matrix(y_test, y_pred_gbc)\n",
        "cm_gbc"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  5,  76],\n",
              "       [  7, 126]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo_tSkJMvh8x",
        "outputId": "13d17bb8-4f85-4dc1-e5fa-32e3e2bc05e9"
      },
      "source": [
        "# Classification report\n",
        "print(\"Report : \\n\", classification_report(y_test, y_pred_gbc))\n",
        "print(\"Accuracy : \",accuracy_score(y_test,y_pred_gbc))\n",
        "accGBC = accuracy_score(y_test,y_pred_gbc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.06      0.11        81\n",
            "           1       0.62      0.95      0.75       133\n",
            "\n",
            "    accuracy                           0.61       214\n",
            "   macro avg       0.52      0.50      0.43       214\n",
            "weighted avg       0.55      0.61      0.51       214\n",
            "\n",
            "Accuracy :  0.6121495327102804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiL67RYJvzQr"
      },
      "source": [
        "## **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgAwR0W0vy3e"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knnc = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Training knn\n",
        "knnc.fit(X_train, y_train)\n",
        "\n",
        "# predict knn to test data\n",
        "y_pred_knnc = knnc.predict(X_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N1e-dFlwRzy",
        "outputId": "9befc5e0-4e2a-4f81-f5c5-6f6f28a0bf84"
      },
      "source": [
        "# Show the Confussion Matrix\n",
        "cm_knnc = confusion_matrix(y_test, y_pred_knnc)\n",
        "cm_knnc"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 54,  27],\n",
              "       [ 10, 123]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiHMEaQSwYMw",
        "outputId": "117b6831-70e8-472f-bf54-9f2f407bcef2"
      },
      "source": [
        "# Classification report\n",
        "print(\"Report : \\n\", classification_report(y_test, y_pred_knnc))\n",
        "print(\"Accuracy : \",accuracy_score(y_test,y_pred_knnc))\n",
        "accKNNC = accuracy_score(y_test,y_pred_knnc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.67      0.74        81\n",
            "           1       0.82      0.92      0.87       133\n",
            "\n",
            "    accuracy                           0.83       214\n",
            "   macro avg       0.83      0.80      0.81       214\n",
            "weighted avg       0.83      0.83      0.82       214\n",
            "\n",
            "Accuracy :  0.8271028037383178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meVSq-v3yxMq"
      },
      "source": [
        "## **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A34azrxGyw5c",
        "outputId": "b7988d56-37e2-4dfc-85c1-144a99e5d5aa"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlpc = MLPClassifier(hidden_layer_sizes=(20, 3), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "\n",
        "# Training knn\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "# predict knn to test data\n",
        "y_pred_mlpc = mlpc.predict(X_test)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.66160988\n",
            "Iteration 2, loss = 0.65714249\n",
            "Iteration 3, loss = 0.65274222\n",
            "Iteration 4, loss = 0.65132911\n",
            "Iteration 5, loss = 0.65144208\n",
            "Iteration 6, loss = 0.65189407\n",
            "Iteration 7, loss = 0.65184266\n",
            "Iteration 8, loss = 0.65029331\n",
            "Iteration 9, loss = 0.64848332\n",
            "Iteration 10, loss = 0.64740101\n",
            "Iteration 11, loss = 0.64683368\n",
            "Iteration 12, loss = 0.64519160\n",
            "Iteration 13, loss = 0.64287492\n",
            "Iteration 14, loss = 0.64009323\n",
            "Iteration 15, loss = 0.63688240\n",
            "Iteration 16, loss = 0.63287599\n",
            "Iteration 17, loss = 0.62720633\n",
            "Iteration 18, loss = 0.62010172\n",
            "Iteration 19, loss = 0.61047877\n",
            "Iteration 20, loss = 0.59801214\n",
            "Iteration 21, loss = 0.58077943\n",
            "Iteration 22, loss = 0.55817764\n",
            "Iteration 23, loss = 0.52975953\n",
            "Iteration 24, loss = 0.49396314\n",
            "Iteration 25, loss = 0.44590564\n",
            "Iteration 26, loss = 0.39189323\n",
            "Iteration 27, loss = 0.34015647\n",
            "Iteration 28, loss = 0.28070260\n",
            "Iteration 29, loss = 0.22830096\n",
            "Iteration 30, loss = 0.18330401\n",
            "Iteration 31, loss = 0.15478074\n",
            "Iteration 32, loss = 0.12045083\n",
            "Iteration 33, loss = 0.09567129\n",
            "Iteration 34, loss = 0.08023437\n",
            "Iteration 35, loss = 0.05086877\n",
            "Iteration 36, loss = 0.04119058\n",
            "Iteration 37, loss = 0.02854300\n",
            "Iteration 38, loss = 0.02159507\n",
            "Iteration 39, loss = 0.01727722\n",
            "Iteration 40, loss = 0.01401996\n",
            "Iteration 41, loss = 0.01163469\n",
            "Iteration 42, loss = 0.00999390\n",
            "Iteration 43, loss = 0.00864149\n",
            "Iteration 44, loss = 0.00764031\n",
            "Iteration 45, loss = 0.00680074\n",
            "Iteration 46, loss = 0.00617169\n",
            "Iteration 47, loss = 0.00566035\n",
            "Iteration 48, loss = 0.00518741\n",
            "Iteration 49, loss = 0.00484180\n",
            "Iteration 50, loss = 0.00449319\n",
            "Iteration 51, loss = 0.00423712\n",
            "Iteration 52, loss = 0.00397135\n",
            "Iteration 53, loss = 0.00376093\n",
            "Iteration 54, loss = 0.00357844\n",
            "Iteration 55, loss = 0.00338769\n",
            "Iteration 56, loss = 0.00324489\n",
            "Iteration 57, loss = 0.00309737\n",
            "Iteration 58, loss = 0.00297172\n",
            "Iteration 59, loss = 0.00285063\n",
            "Iteration 60, loss = 0.00276493\n",
            "Iteration 61, loss = 0.00264102\n",
            "Iteration 62, loss = 0.00255336\n",
            "Iteration 63, loss = 0.00246264\n",
            "Iteration 64, loss = 0.00238002\n",
            "Iteration 65, loss = 0.00230378\n",
            "Iteration 66, loss = 0.00223364\n",
            "Iteration 67, loss = 0.00217098\n",
            "Iteration 68, loss = 0.00210088\n",
            "Iteration 69, loss = 0.00204413\n",
            "Iteration 70, loss = 0.00198531\n",
            "Iteration 71, loss = 0.00193678\n",
            "Iteration 72, loss = 0.00188137\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "159792vczUiv",
        "outputId": "3877a577-4b85-48d1-9579-b1bb2d16e488"
      },
      "source": [
        "# Show the Confussion Matrix\n",
        "cm_mlpc = confusion_matrix(y_test, y_pred_mlpc)\n",
        "cm_mlpc"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 57,  24],\n",
              "       [  8, 125]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIzl_AtAzeUm",
        "outputId": "bdc5de50-9964-4058-e6b5-97169ed0ef04"
      },
      "source": [
        "# Classification report\n",
        "print(\"Report : \\n\", classification_report(y_test, y_pred_mlpc))\n",
        "print(\"Accuracy : \",accuracy_score(y_test,y_pred_mlpc))\n",
        "accMLPC = accuracy_score(y_test,y_pred_mlpc)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.70      0.78        81\n",
            "           1       0.84      0.94      0.89       133\n",
            "\n",
            "    accuracy                           0.85       214\n",
            "   macro avg       0.86      0.82      0.83       214\n",
            "weighted avg       0.85      0.85      0.85       214\n",
            "\n",
            "Accuracy :  0.8504672897196262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G74bk5HN3SxK"
      },
      "source": [
        "## **Model Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ipkK_YJG3SdZ",
        "outputId": "c716e91b-8bc1-49c1-9f85-92d7ded87fca"
      },
      "source": [
        "# Accuracy Comparison\n",
        "model = ['SVM', 'Logistic Regression', 'Gradient Boosting', 'KNN', 'MLP']\n",
        "accuracies = [accSVMC, accLRC, accGBC, accKNNC, accMLPC]\n",
        "comp = pd.DataFrame(list(zip(model, accuracies)), columns=['Model', 'Accuracy'])\n",
        "comp"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.775701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.757009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.612150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.827103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.850467</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy\n",
              "0                  SVM  0.775701\n",
              "1  Logistic Regression  0.757009\n",
              "2    Gradient Boosting  0.612150\n",
              "3                  KNN  0.827103\n",
              "4                  MLP  0.850467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dSdRph5df7"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Based on models accuracy above, we can conclude that the best model is MLP because it has biggest accuracy than other "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7wBaZ4763Sa"
      },
      "source": [
        "# **Predict Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV-9PWF473Ps"
      },
      "source": [
        "df_hs.to_csv('HS dataset Clean.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz38r6zL62PD",
        "outputId": "937605d3-21da-42ad-b51d-ea7596bc611f"
      },
      "source": [
        "# Input New Statement\n",
        "new_statement = ['terima kasih pak sudah abdi dijakarta dengan gudang minta warga','mungkin ada kata kata yang sakit ya pak tapi moga itu jadi semangat untuk bapak depan',\n",
        "                 'tahu anda apa sama antara malin kundang dengan ahok mereka dua sama sam durhaka asbak iklanahokjahat kampanyeahokjahat',\n",
        "                 'ngomong aja ente sama kaca pecah belah rukun lagi hok ente bisa ngaco iklanahokjahat',\n",
        "                 'pala otak kau pecah tapi ada orang baik yang tolong','sungguh jahat anda terhadap dia', \n",
        "                 'ngomong aja ga jelas seperti anjing yang menggonggong', 'bilamana saya marah saya akan minum es', \n",
        "                 'apa perbedaan anda dengan malin kundang apabila anda mengutuk ibu anda', 'lela bukan anak yang baik'] \n",
        "\n",
        "# Extract Features\n",
        "new_statement_features = tfidf_vectorizer.transform(new_statement).toarray()\n",
        "\n",
        "## encodeing predict class\n",
        "predict_sentiment = encoder.inverse_transform(mlpc.predict(new_statement_features))\n",
        "print('sentiment: ',predict_sentiment)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment:  ['Non_HS' 'Non_HS' 'HS' 'HS' 'Non_HS' 'Non_HS' 'HS' 'Non_HS' 'HS' 'Non_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}